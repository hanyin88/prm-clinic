import json
import math
from collections import defaultdict
import statistics
import pandas as pd
import argparse
import os

def main():
    # Argument parsing
    parser = argparse.ArgumentParser(description='Calculate accuracy for model predictions.')
    parser.add_argument('--data_file', type=str, required=True, help='Path to the JSON file generated by forward_pass.py.')
    parser.add_argument('--gold_labels_file', type=str, required=True, help='Path to the CSV file containing gold labels.')
    parser.add_argument('--results_dir', type=str, required=True, help='Directory to save the results.')
    parser.add_argument('--model_name', type=str, required=True, help='Model checkpoint names.')
    args = parser.parse_args()

    # Load data from JSON file
    with open(args.data_file, 'r') as f:
        data = json.load(f)
        
# Read gold labels
    gold_labels_df = pd.read_csv(args.gold_labels_file)
    gold_labels_df = gold_labels_df.set_index('Case_id')

    # Prepare dictionaries to hold predictions
    prm_predictions = {}  # PRM method predictions
    orm_predictions = {}  # ORM method predictions
    
    # Read model name
    model_name = args.model_name

    # Group items by ('Case_id', 'Sample_no')
    groups = defaultdict(list)
    for item in data:
        key = (item['Case_id'], item['Sample_no'])
        groups[key].append(item)

    # Prepare a dictionary to hold Case_id and their corresponding samples
    case_samples = defaultdict(list)

    # Compute sum of log probabilities for PRM method
    for key, items in groups.items():
        Case_id, Sample_no = key
        plus_probs = [float(item['plus_prob']) for item in items]
        # Ensure probabilities are greater than 0 to avoid math domain error
        plus_probs = [p if p > 0 else 1e-10 for p in plus_probs]
        log_plus_probs = [math.log(p) for p in plus_probs]
        sum_log_plus_prob = sum(log_plus_probs)
        case_samples[Case_id].append({
            'Sample_no': Sample_no,
            'sum_log_plus_prob': sum_log_plus_prob
        })

    # Determine the sample with the highest probability for each Case_id (PRM method)
    for Case_id, samples in case_samples.items():
        # Sort samples by sum of log probabilities in descending order
        samples.sort(key=lambda x: x['sum_log_plus_prob'], reverse=True)
        top_sample = samples[0]
        Sample_no = top_sample['Sample_no']
        prm_predictions[Case_id] = Sample_no

    # ORM method
    # For each Case_id, find the sample where 'preceding_token' == "<|reserved_special_token_5|>" with highest 'log_plus_prob'

    # Prepare a dictionary to hold ORM method data
    orm_case_samples = defaultdict(list)

    for item in data:
        if item['preceding_token'] == "<|reserved_special_token_5|>":
            Case_id = item['Case_id']
            Sample_no = item['Sample_no']
            plus_prob = float(item['plus_prob'])
            plus_prob = plus_prob if plus_prob > 0 else 1e-10
            log_plus_prob = math.log(plus_prob)
            orm_case_samples[Case_id].append({
                'Sample_no': Sample_no,
                'log_plus_prob': log_plus_prob
            })

    # Determine the sample with the highest log_plus_prob for each Case_id (ORM method)
    for Case_id, samples in orm_case_samples.items():
        # Sort samples by log_plus_prob in descending order
        samples.sort(key=lambda x: x['log_plus_prob'], reverse=True)
        top_sample = samples[0]
        Sample_no = top_sample['Sample_no']
        orm_predictions[Case_id] = Sample_no

    # # Read gold labels
    # gold_labels_df = pd.read_csv('prm_project/dataset/6_llama_clinic_note_to_json_via_python/rlhf_preferred_notes.csv')  # Replace with your actual gold labels CSV filename
    # gold_labels_df = gold_labels_df.set_index('Case_id')

    # Initialize counters
    prm_correct = 0
    orm_correct = 0
    total_cases = 0

    # Prepare results for saving
    results = []

    for Case_id in prm_predictions.keys():
        if Case_id in gold_labels_df.index:
            Preferred_note = gold_labels_df.loc[Case_id, 'Preferred_note']  # Assuming Preferred_note matches 'Sample_no' values
        else:
            print(f"Warning: Case_id {Case_id} not found in gold labels.")
            continue

        total_cases += 1

        prm_pred = prm_predictions[Case_id]
        prm_is_correct = int(prm_pred == Preferred_note)
        prm_correct += prm_is_correct

        orm_pred = orm_predictions.get(Case_id, None)
        # In case ORM method did not produce a prediction for this Case_id
        if orm_pred is not None:
            orm_is_correct = int(orm_pred == Preferred_note)
            orm_correct += orm_is_correct
        else:
            orm_pred = 'N/A'
            orm_is_correct = 0  # Cannot be correct if no prediction
            

        results.append({
            'Case_id': Case_id,
            'Preferred_note': Preferred_note,
            'PRM_prediction': prm_pred,
            'PRM_correct': prm_is_correct,
            'ORM_prediction': orm_pred,
            'ORM_correct': orm_is_correct
        })

    # Calculate accuracies
    prm_accuracy = (prm_correct / total_cases) * 100 if total_cases > 0 else 0
    orm_accuracy = (orm_correct / total_cases) * 100 if total_cases > 0 else 0
    
    # Save results to CSV
    results_df = pd.DataFrame(results)
    results_df.to_csv(os.path.join(args.results_dir, f'{model_name}_accuracy_results.csv'), index=False)

    # Save overall accuracies
    with open(os.path.join(args.results_dir, f'{model_name}_accuracy_summary.csv'), 'w') as f:
        f.write('Method,Accuracy\n')
        f.write(f'PRM_accuracy,{prm_accuracy:.2f}%\n')
        f.write(f'ORM_accuracy,{orm_accuracy:.2f}%\n')

    print(f"PRM Accuracy: {prm_accuracy:.2f}%")
    print(f"ORM Accuracy: {orm_accuracy:.2f}%")
    print("Results saved to 'accuracy_results.csv' and 'accuracy_summary.csv'.")

if __name__ == '__main__':
    main()
